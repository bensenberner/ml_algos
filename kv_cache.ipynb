{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e2f12d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' \\nKV CACHE FUNDAMENTALS. WHAT EVEN IS THIS BOI? \\n    - Basically its just a preallocated tensor that we fill up during autoregressive inference (\"decoding\")\\n        - We use it to avoid recomputing (K, V)\\n    - It naturally leads to a two-stage interpretation fo inference \\n        - The first stage takes in a full prompt and populates the empty cache, this is called \"prefill\" \\n        - The second stage passes in ONLY ONE TOKEN IN THE FORWARD PASS AT A TIME, \\n            because the information from all past tokens IS ENTIRELY CAPTURED IN THE KV CACHE \\n            - You can think of the KV cache as the \"state\" of a transformer, ie. the sufficient summary statistic \\n            of all past tokens to generate the next token. Unlike RNN/SSM state, its size grows over time\\n            (hence why Transformers perform better, they do zero compression of their inputs, but also take more memory)\\n        - The second stage is called decoding, and we basically add [k_t, v_t] to update the cache\\n        as we do a forward with each token one at a time, and then use the updated cache as our K, V matrices\\n        for attention. \\n    - Common question, why don\\'t we cache queries, Q? \\n        - Answer: this is a good question and important to make sure you understand. The reason is because KV cache is a \\n        SUFFICIENT SUMMARY STATISTIC OF THE PAST TOKENS, and we DO NOT need to know the queries of past tokens to compute \\n        the attention pattern of a SINGLE NEW TOKEN during decoding, only the query of the new token. \\n        However, we DO need the keys and values of previous tokens, as the addition to the attention matrix for this \\n        new token will be something like (q_t @ all_past_keys.T) @ all_past_values. \\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' \n",
    "KV CACHE FUNDAMENTALS. WHAT EVEN IS THIS BOI? \n",
    "    - Basically its just a preallocated tensor that we fill up during autoregressive inference (\"decoding\")\n",
    "        - We use it to avoid recomputing (K, V)\n",
    "    - It naturally leads to a two-stage interpretation fo inference \n",
    "        - The first stage takes in a full prompt and populates the empty cache, this is called \"prefill\" \n",
    "        - The second stage passes in ONLY ONE TOKEN IN THE FORWARD PASS AT A TIME, \n",
    "            because the information from all past tokens IS ENTIRELY CAPTURED IN THE KV CACHE \n",
    "            - You can think of the KV cache as the \"state\" of a transformer, ie. the sufficient summary statistic \n",
    "            of all past tokens to generate the next token. Unlike RNN/SSM state, its size grows over time\n",
    "            (hence why Transformers perform better, they do zero compression of their inputs, but also take more memory)\n",
    "        - The second stage is called decoding, and we basically add [k_t, v_t] to update the cache\n",
    "        as we do a forward with each token one at a time, and then use the updated cache as our K, V matrices\n",
    "        for attention. \n",
    "    - Common question, why don't we cache queries, Q? \n",
    "        - Answer: this is a good question and important to make sure you understand. The reason is because KV cache is a \n",
    "        SUFFICIENT SUMMARY STATISTIC OF THE PAST TOKENS, and we DO NOT need to know the queries of past tokens to compute \n",
    "        the attention pattern of a SINGLE NEW TOKEN during decoding, only the query of the new token. \n",
    "        However, we DO need the keys and values of previous tokens, as the addition to the attention matrix for this \n",
    "        new token will be something like (q_t @ all_past_keys.T) @ all_past_values. \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d573e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from typing import List\n",
    "\n",
    "ACT2FN = {\n",
    "    'relu': torch.nn.functional.relu,\n",
    "    'gelu': torch.nn.functional.gelu,\n",
    "    'silu': torch.nn.functional.silu,\n",
    "    'swish': torch.nn.functional.silu,\n",
    "}\n",
    "\n",
    "class Attention(torch.nn.Module): # BSD -> BSD\n",
    "    def __init__(self, D=768, layer_idx=None, head_dim=64, causal=True, device=\"cuda\", gqa=False): \n",
    "        super().__init__()\n",
    "        self.D = D \n",
    "        self.head_dim = head_dim\n",
    "        self.gqa = gqa \n",
    "        assert D % head_dim == 0\n",
    "        self.nheads = D//head_dim\n",
    "        self.Wq = torch.nn.Linear(D, D)\n",
    "        self.Wk = torch.nn.Linear(D, D)\n",
    "        self.Wv = torch.nn.Linear(D, D)\n",
    "        self.causal = causal \n",
    "        self.Wo = torch.nn.Linear(D, D)\n",
    "        self.device = device\n",
    "        self.layer_idx = layer_idx\n",
    "\n",
    "    def forward(self, x: torch.Tensor, kv_cache): # input is [B, S, D] \n",
    "        B, S, D = x.shape\n",
    "        # let's make this multi-head now, ie. make each QKV [B, S, D] --> [B, nh, S, hd]\n",
    "\n",
    "        Q, K, V = self.Wq(x), self.Wk(x), self.Wv(x) # all [B, S, D]\n",
    "\n",
    "        Q = Q.view(B, S, self.nheads, self.head_dim).transpose(1,2) # [B, nh, S, hd]\n",
    "        K = K.view(B, S, self.nheads, self.head_dim).transpose(1,2) \n",
    "        V = V.view(B, S, self.nheads, self.head_dim).transpose(1,2)\n",
    "\n",
    "        # update kv cache \n",
    "        \n",
    "        layer_idx = self.layer_idx \n",
    "        if kv_cache is not None and layer_idx is not None: \n",
    "            ## TODO: implement!!\n",
    "            pass\n",
    "\n",
    "        # [B, nh, S, hd] @ [B, nh, hd, S] -> [B, nh, S, S]\n",
    "        scale = torch.sqrt(torch.tensor(self.head_dim, dtype=Q.dtype, device=self.device))\n",
    "        logits = (Q @ K.transpose(-2, -1)) / scale\n",
    "        if self.causal:\n",
    "            mask = torch.triu(torch.ones_like(logits), diagonal=1).bool()\n",
    "            logits_masked = logits.masked_fill(mask, float('-inf'))\n",
    "        else:\n",
    "            logits_masked = logits\n",
    "\n",
    "        A = torch.nn.functional.softmax(logits_masked, dim=-1) # [B, nh, S, S]\n",
    "        \n",
    "        preout = torch.einsum('bnxy,bnyd->bnxd', A, V) # [B, nh, S, S] @ [B, nh, S, hd] -> [B, nh, S, hd]\n",
    "        preout = preout.transpose(1, 2).reshape(B, S, -1) # [B, nh, S, hd] -> [B, S, nh * hd]\n",
    "        \n",
    "        out = self.Wo(preout) # [B, S, D]\n",
    "        return out # [B, S, D]\n",
    "\n",
    "class MLP(torch.nn.Module): \n",
    "    def __init__(self, D, hidden_multiplier=4, act='swish', device=None): \n",
    "        super().__init__()\n",
    "        self.D = D\n",
    "        self.device = device if device is not None else torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.up_proj = torch.nn.Linear(D, D*hidden_multiplier)\n",
    "        self.down_proj = torch.nn.Linear(D*hidden_multiplier, D)\n",
    "        self.act = ACT2FN[act]\n",
    "\n",
    "    def forward(self, x): # BSD -> BSD automatically on last dim \n",
    "        return self.down_proj(self.act(self.up_proj(x)))\n",
    "\n",
    "class LN(torch.nn.Module): \n",
    "    def __init__(self, D, eps=1e-9, device=None): \n",
    "        super().__init__()\n",
    "        self.D = D \n",
    "        self.eps = eps\n",
    "        self.device = device if device is not None else torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.mean_scale = torch.nn.Parameter(torch.zeros(D))\n",
    "        self.std_scale = torch.nn.Parameter(torch.ones(D))\n",
    "\n",
    "    def forward(self, x): # x is [B, S, D]\n",
    "        mean = x.mean(dim=-1, keepdim=True) # [B, S, 1]\n",
    "        std = (x.var(dim=-1, keepdim=True) + self.eps)**0.5 # [B, S, 1]\n",
    "        x_norm = (x - mean)/(std) \n",
    "        return x_norm * self.std_scale + self.mean_scale\n",
    "\n",
    "class TransformerLayer(torch.nn.Module): \n",
    "    def __init__(self, D, gqa=False, device=None): \n",
    "        super().__init__()\n",
    "        self.D = D \n",
    "        self.device = device if device is not None else torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.attn = Attention(D, gqa=gqa, device=self.device)\n",
    "        self.mlp = MLP(D, device=self.device)\n",
    "        self.ln1 = LN(D, device=self.device)\n",
    "        self.ln2 = LN(D, device=self.device)  \n",
    "    \n",
    "    def forward(self, x, kv_cache=None): # x is BSD\n",
    "        ln1_out = self.ln1(x)\n",
    "        attn_out = self.attn(ln1_out, kv_cache=kv_cache)\n",
    "        x = x + attn_out\n",
    "        ln2_out = self.ln2(x)\n",
    "        mlp_out = self.mlp(ln2_out)\n",
    "        x = x + mlp_out\n",
    "        return x \n",
    "\n",
    "class PositionalEmbedding(torch.nn.Module):\n",
    "    def __init__(self, max_seq_len, D, device=None):\n",
    "        super().__init__()\n",
    "        self.device = device if device is not None else torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.pos_embedding = torch.nn.Parameter(torch.randn(max_seq_len, D))\n",
    "    \n",
    "    def forward(self, x): # x is [B, S, D]\n",
    "        B, S, D = x.shape\n",
    "        return x + self.pos_embedding[:S] # Broadcasting handles batch dimension\n",
    "\n",
    "class EmbeddingLayer(torch.nn.Module): \n",
    "    # this is just a lookup table \n",
    "    def __init__(self, vocab_size, D, device=None): \n",
    "        super().__init__()\n",
    "        self.device = device if device is not None else torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.embedding = torch.nn.Parameter(torch.randn(vocab_size, D))\n",
    "\n",
    "    def forward(self, x): \n",
    "        return self.embedding[x]\n",
    "\n",
    "class UnembeddingLayer(torch.nn.Module): \n",
    "    # this is just a lookup table that maps embeddings back to logits\n",
    "    def __init__(self, vocab_size, D, device=None): \n",
    "        super().__init__()\n",
    "        self.device = device if device is not None else torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.unembedding = torch.nn.Linear(D, vocab_size)\n",
    "\n",
    "    def forward(self, x): # x is [B, S, D]\n",
    "        # Return logits of shape [B, S, vocab_size]\n",
    "        return self.unembedding(x)\n",
    "\n",
    "class Transformer(torch.nn.Module): \n",
    "    def __init__(self, depth, hidden_dim, vocab_size, max_seq_len=16384, device=None, gqa=False): \n",
    "        super().__init__()\n",
    "        self.depth = depth\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.emb = EmbeddingLayer(vocab_size, hidden_dim, device=device)\n",
    "        self.pos_emb = PositionalEmbedding(max_seq_len, hidden_dim, device=device)\n",
    "        self.unemb = UnembeddingLayer(vocab_size, hidden_dim, device=device)\n",
    "        self.gqa = gqa \n",
    "        self.layers = torch.nn.ModuleList([TransformerLayer(hidden_dim, gqa, device=device) for _ in range(depth)])\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            layer.attn.layer_idx = i  \n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, x, kv_cache=None):\n",
    "        x = self.emb(x)\n",
    "        if kv_cache is not None:\n",
    "            # When decoding, only add positional embeddings for the new tokens.\n",
    "            pos_offset = kv_cache.current_length\n",
    "            pos_emb = self.pos_emb.pos_embedding[pos_offset: pos_offset + x.size(1)].unsqueeze(0)\n",
    "            x = x + pos_emb\n",
    "        else:\n",
    "            x = self.pos_emb(x)\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            x = layer(x, kv_cache=kv_cache)\n",
    "        x = self.unemb(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f244d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KVCache:\n",
    "    \"\"\"\n",
    "    minimal kv cache for transformer models that preallocates memory for k/v pairs.\n",
    "    k/v tensors have shape: [batch_size, num_heads, max_seq_len, head_dim]\n",
    "    \"\"\"\n",
    "    def __init__(self, num_layers: int, batch_size: int, num_heads: int, head_dim: int, max_seq_len: int, device='cuda'):\n",
    "        self.num_layers = num_layers\n",
    "        self.batch_size = batch_size\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = head_dim\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.device = device\n",
    "        self.current_length = 0\n",
    "        # preallocate cache memory for each layer\n",
    "        self.keys = [\n",
    "            torch.empty(batch_size, num_heads, max_seq_len, head_dim, device=device)\n",
    "            for _ in range(num_layers)\n",
    "        ]\n",
    "        self.values = [\n",
    "            torch.empty(batch_size, num_heads, max_seq_len, head_dim, device=device)\n",
    "            for _ in range(num_layers)\n",
    "        ]\n",
    "\n",
    "    def update(self, layer: int, new_keys: torch.Tensor, new_values: torch.Tensor):\n",
    "        \"\"\"\n",
    "        updates cache for a layer with new k/v tensors of shape:\n",
    "        [batch_size, num_heads, token_count, head_dim]\n",
    "        \"\"\"\n",
    "        # TODO: implement!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d8cce4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerGenerator:\n",
    "    \"\"\"\n",
    "    fast autoregressive inference using preallocated kv cache. model must accept\n",
    "    'kv_cache' arg in forward() to load/append k/v pairs from cache\n",
    "    \"\"\"\n",
    "    def __init__(self, model: Transformer, max_seq_len: int=4096):\n",
    "        self.model = model\n",
    "        self.device = next(model.parameters()).device\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.kv_cache = None\n",
    "\n",
    "    def _initialize_cache(self, batch_size: int):\n",
    "        # extract attn params from first layer to set up cache dims\n",
    "        first_layer = self.model.layers[0]\n",
    "        attn = first_layer.attn\n",
    "        \n",
    "        num_heads = attn.num_kv_heads if (hasattr(attn, 'gqa') and attn.gqa) else attn.nheads\n",
    "        head_dim = attn.head_dim\n",
    "        \n",
    "        self.kv_cache = self.model.kv_cache = KVCache(\n",
    "            self.model.depth, \n",
    "            batch_size, \n",
    "            num_heads,\n",
    "            head_dim,\n",
    "            self.max_seq_len, \n",
    "            self.device\n",
    "        )\n",
    "\n",
    "    def _prefill(self, prompt: List[int]):\n",
    "        # process initial prompt (shape [1, seq_len]) to fill kv cache\n",
    "        prompt_tensor = torch.tensor(prompt, device=self.device).unsqueeze(0)\n",
    "        batch_size = prompt_tensor.size(0)\n",
    "        self._initialize_cache(batch_size)\n",
    "        _ = self.model(prompt_tensor, kv_cache=self.kv_cache)\n",
    "        self.kv_cache.current_length = prompt_tensor.size(1)\n",
    "        return self.kv_cache\n",
    "\n",
    "    def generate(self, prompt: List[int], max_new_tokens: int):\n",
    "        \"\"\"\n",
    "        generates tokens autoregressively given a prompt\n",
    "        returns: list[int] with prompt + new tokens\n",
    "        \"\"\"\n",
    "        kv_cache = self._prefill(prompt)\n",
    "        generated = list(prompt)\n",
    "        for _ in range(max_new_tokens):\n",
    "            # feed last token through model, shape [1,1]\n",
    "            input_tensor = torch.tensor([[generated[-1]]], device=self.device)\n",
    "            logits = self.model(input_tensor, kv_cache=kv_cache)\n",
    "            next_token = int(torch.argmax(logits[:, -1, :], dim=-1).item())\n",
    "            generated.append(next_token)\n",
    "            kv_cache.current_length += 1\n",
    "        return generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "20bc53f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4 [00:06<?, ?it/s]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'KVCache' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 49\u001b[39m\n\u001b[32m     46\u001b[39m times_for_this_n_no_kv.append((end_time_no_kv - start_time_no_kv) * \u001b[32m1000\u001b[39m)\n\u001b[32m     48\u001b[39m start_time_kv = time.time()\n\u001b[32m---> \u001b[39m\u001b[32m49\u001b[39m _ = \u001b[43mgenerator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_tokens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     50\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m device == \u001b[33m\"\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m     51\u001b[39m     torch.cuda.synchronize()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 43\u001b[39m, in \u001b[36mTransformerGenerator.generate\u001b[39m\u001b[34m(self, prompt, max_new_tokens)\u001b[39m\n\u001b[32m     38\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgenerate\u001b[39m(\u001b[38;5;28mself\u001b[39m, prompt: List[\u001b[38;5;28mint\u001b[39m], max_new_tokens: \u001b[38;5;28mint\u001b[39m):\n\u001b[32m     39\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     40\u001b[39m \u001b[33;03m    generates tokens autoregressively given a prompt\u001b[39;00m\n\u001b[32m     41\u001b[39m \u001b[33;03m    returns: list[int] with prompt + new tokens\u001b[39;00m\n\u001b[32m     42\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m43\u001b[39m     kv_cache = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_prefill\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     44\u001b[39m     generated = \u001b[38;5;28mlist\u001b[39m(prompt)\n\u001b[32m     45\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_new_tokens):\n\u001b[32m     46\u001b[39m         \u001b[38;5;66;03m# feed last token through model, shape [1,1]\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 33\u001b[39m, in \u001b[36mTransformerGenerator._prefill\u001b[39m\u001b[34m(self, prompt)\u001b[39m\n\u001b[32m     31\u001b[39m prompt_tensor = torch.tensor(prompt, device=\u001b[38;5;28mself\u001b[39m.device).unsqueeze(\u001b[32m0\u001b[39m)\n\u001b[32m     32\u001b[39m batch_size = prompt_tensor.size(\u001b[32m0\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m33\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_initialize_cache\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     34\u001b[39m _ = \u001b[38;5;28mself\u001b[39m.model(prompt_tensor, kv_cache=\u001b[38;5;28mself\u001b[39m.kv_cache)\n\u001b[32m     35\u001b[39m \u001b[38;5;28mself\u001b[39m.kv_cache.current_length = prompt_tensor.size(\u001b[32m1\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 20\u001b[39m, in \u001b[36mTransformerGenerator._initialize_cache\u001b[39m\u001b[34m(self, batch_size)\u001b[39m\n\u001b[32m     17\u001b[39m num_heads = attn.num_kv_heads \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mhasattr\u001b[39m(attn, \u001b[33m'\u001b[39m\u001b[33mgqa\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m attn.gqa) \u001b[38;5;28;01melse\u001b[39;00m attn.nheads\n\u001b[32m     18\u001b[39m head_dim = attn.head_dim\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m \u001b[38;5;28mself\u001b[39m.kv_cache = \u001b[38;5;28mself\u001b[39m.model.kv_cache = \u001b[43mKVCache\u001b[49m(\n\u001b[32m     21\u001b[39m     \u001b[38;5;28mself\u001b[39m.model.depth, \n\u001b[32m     22\u001b[39m     batch_size, \n\u001b[32m     23\u001b[39m     num_heads,\n\u001b[32m     24\u001b[39m     head_dim,\n\u001b[32m     25\u001b[39m     \u001b[38;5;28mself\u001b[39m.max_seq_len, \n\u001b[32m     26\u001b[39m     \u001b[38;5;28mself\u001b[39m.device\n\u001b[32m     27\u001b[39m )\n",
      "\u001b[31mNameError\u001b[39m: name 'KVCache' is not defined"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "# this profiling takes ~10min on a single H100, you can reduce any of the below to make it faster \n",
    "# (hidden_dim, num_layers, input_seq_len, num_decode_tokens, n_runs)\n",
    "vocab_size = 16\n",
    "hidden_dim = 64\n",
    "n_layers = 6\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model = Transformer(depth=n_layers, hidden_dim=hidden_dim, vocab_size=vocab_size, device=device)\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "generator = TransformerGenerator(model)\n",
    "\n",
    "input_seq_len = 512\n",
    "num_decode_tokens = [512, 2048, 4096, 8192]\n",
    "decode_times_no_kv = []\n",
    "decode_times_kv = []\n",
    "n_runs = 1\n",
    "\n",
    "with torch.no_grad():\n",
    "    for n_tokens in tqdm(num_decode_tokens):\n",
    "        batch_size = 1\n",
    "        times_for_this_n_no_kv = []\n",
    "        times_for_this_n_kv = []\n",
    "\n",
    "        for _ in range(n_runs):\n",
    "            prompt_tensor = torch.randint(0, vocab_size, (batch_size, input_seq_len)).to(device)\n",
    "            prompt_list = prompt_tensor.squeeze(0).tolist()\n",
    "\n",
    "            x = prompt_tensor.clone()\n",
    "            start_time_no_kv = time.time()\n",
    "            for _ in range(n_tokens):\n",
    "                logits = model(x)\n",
    "                # get next token prediction [B, 1]\n",
    "                next_token = torch.argmax(logits[:, -1:, :], dim=-1)\n",
    "                x = torch.cat([x, next_token], dim=1)\n",
    "            if device == \"cuda\":\n",
    "                torch.cuda.synchronize()\n",
    "            end_time_no_kv = time.time()\n",
    "            times_for_this_n_no_kv.append((end_time_no_kv - start_time_no_kv) * 1000)\n",
    "\n",
    "            start_time_kv = time.time()\n",
    "            _ = generator.generate(prompt_list, n_tokens)\n",
    "            if device == \"cuda\":\n",
    "                torch.cuda.synchronize()\n",
    "            end_time_kv = time.time()\n",
    "            times_for_this_n_kv.append((end_time_kv - start_time_kv) * 1000)\n",
    "\n",
    "        avg_time_no_kv = np.mean(times_for_this_n_no_kv)\n",
    "        decode_times_no_kv.append(avg_time_no_kv)\n",
    "        avg_time_kv = np.mean(times_for_this_n_kv)\n",
    "        decode_times_kv.append(avg_time_kv)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(num_decode_tokens, decode_times_no_kv, marker='o', label='No KV Cache')\n",
    "plt.plot(num_decode_tokens, decode_times_kv, marker='s', label='With KV Cache')\n",
    "plt.xlabel('Number of Decoded Tokens')\n",
    "plt.ylabel('Total Time (ms)')\n",
    "plt.title(f'Decoding Latency vs Number of Generated Tokens\\n(Input Length: {input_seq_len}, Averaged over {n_runs} runs)')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.yscale('log')\n",
    "plt.xscale('log')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nAverage time per decoded token (averaged over {n_runs} runs):\")\n",
    "print(\"----------------------------------------------------\")\n",
    "print(\"Decoding Method | Tokens Decoded | Avg ms/token\")\n",
    "print(\"----------------------------------------------------\")\n",
    "for n_tokens, total_time_no_kv, total_time_kv in zip(num_decode_tokens, decode_times_no_kv, decode_times_kv):\n",
    "    avg_per_token_no_kv = total_time_no_kv / n_tokens if n_tokens > 0 else 0\n",
    "    avg_per_token_kv = total_time_kv / n_tokens if n_tokens > 0 else 0\n",
    "    print(f\"No KV Cache     | {n_tokens:14d} | {avg_per_token_no_kv:12.2f}\")\n",
    "    print(f\"With KV Cache   | {n_tokens:14d} | {avg_per_token_kv:12.2f}\")\n",
    "    print(\"----------------------------------------------------\")\n",
    "\n",
    "# Hurray! We can see that without KV cache, decoding latency is nonlinear (it will be quadratic if you zoom out more)\n",
    "# but with KV cache, decoding latency is linear in sequence length, so this is a O(S) big win!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a73cddf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
