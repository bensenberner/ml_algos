{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "76bcce0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints\n",
    "(https://arxiv.org/pdf/2305.13245)\n",
    "\n",
    "The TLDR of GQA vs normal MHSA is that storing lots of keys and values for many heads \n",
    "takes up a lot of memory, so we just share keys and values across multiple heads, grouping\n",
    "the keys/vals for a layer by the queries (which we keep one set of per head)\n",
    "so the MHSA is now \"grouped\" and less expensive, especially for longer sequences\n",
    "when the memory required to store lots of keys and values (see KV cache) grows large. \n",
    "'''\n",
    "import torch as t\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "import einops\n",
    "from jaxtyping import Float\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "34f5a16c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MHA(nn.Module):\n",
    "    def __init__(self, d_model, d_head, num_heads, num_groups):\n",
    "        super().__init__()\n",
    "        self.d_head = d_head\n",
    "        self.num_head = num_heads\n",
    "        # self.num_groups = num_groups\n",
    "        # TODO: why Linear vs Parameter\n",
    "        # shape: (num_heads, d_model, d_head)\n",
    "        self.W_Q = nn.Parameter(\n",
    "            t.randn((num_heads, d_model, d_head))\n",
    "        )\n",
    "        self.W_K = nn.Parameter(\n",
    "            t.randn(num_heads, d_model, d_head)\n",
    "        )\n",
    "        self.W_V = nn.Parameter(\n",
    "            t.randn(num_heads, d_model, d_head)\n",
    "        )\n",
    "        self.W_O = nn.Parameter(\n",
    "            t.randn(num_heads * d_head, d_model)\n",
    "        )\n",
    "        \n",
    "    def forward(self, embed: Float[t.Tensor, \"batch seq_len d_model\"]) -> Float[t.Tensor, \"batch seq_len d_model\"]:\n",
    "        Q = einops.einsum(embed, self.W_Q, \"b s dm, nh dm dh -> b nh s dh\")\n",
    "        K = einops.einsum(embed, self.W_K, \"b s dm, nh dm dh -> b nh s dh\")\n",
    "        V = einops.einsum(embed, self.W_V, \"b s dm, nh dm dh -> b nh s dh\")\n",
    "        # head_to_group = t.arange(0, self.num_head) % self.num_groups\n",
    "        K_t = einops.rearrange(K, \"b nh s dh -> b nh dh s\")\n",
    "        Q_K = (\n",
    "            einops.einsum(Q, K_t, \"b nh s_q dh, b nh dh s_k -> b nh s_q s_k\")\n",
    "            / t.sqrt(t.tensor(self.d_head))\n",
    "        )\n",
    "        softmaxed = t.softmax(Q_K, dim=-1)\n",
    "        # TODO: softmax?\n",
    "        \"\"\"\n",
    "        Q = b, n_q, d_head\n",
    "        K = b d_head n_k\n",
    "        b n_q n_k\n",
    "        \"\"\"\n",
    "        # RuntimeError: einsum(): subscript b has size 2 for operand 1 which does not broadcast with previously seen size 4\n",
    "        attn = einops.einsum(softmaxed, V, \"b nh s_q s_k,  b nh s_k dh -> b nh s_q dh\")\n",
    "        attn_unraveled = einops.rearrange(attn, \"b nh s_q dh -> b s_q (nh dh)\") # TODO: is this order correct?\n",
    "        output = einops.einsum(attn_unraveled, self.W_O, \"b s_q nh_dh, nh_dh dm -> b s_q dm\")\n",
    "        \n",
    "        expected = F.scaled_dot_product_attention(\n",
    "            query=Q,\n",
    "            key=K,\n",
    "            value=V,\n",
    "        )\n",
    "        print(expected.shape, attn.shape)\n",
    "        assert t.allclose(expected, attn)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d04eb857",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GQA(nn.Module):\n",
    "    def __init__(self, d_model, d_head, num_heads, num_groups):\n",
    "        super().__init__()\n",
    "        assert num_heads % num_groups == 0\n",
    "        self.d_model = d_model\n",
    "        self.d_head = d_head\n",
    "        self.num_heads = num_heads\n",
    "        self.num_groups = num_groups\n",
    "        # TODO: why Linear vs Parameter\n",
    "        # shape: (num_heads, d_model, d_head)\n",
    "        self.W_Q = nn.Parameter(\n",
    "            t.randn((num_heads, d_model, d_head))\n",
    "        )\n",
    "        self.W_K = nn.Parameter(\n",
    "            t.randn(num_groups, d_model, d_head)\n",
    "        )\n",
    "        self.W_V = nn.Parameter(\n",
    "            t.randn(num_groups, d_model, d_head)\n",
    "        )\n",
    "        self.W_O = nn.Parameter(\n",
    "            t.randn(num_heads * d_head, d_model)\n",
    "        )\n",
    "        \n",
    "    def forward(self, embed: Float[t.Tensor, \"batch seq_len d_model\"]) -> Float[t.Tensor, \"batch seq_len d_model\"]:\n",
    "        Q = einops.einsum(embed, self.W_Q, \"b s dm, nh dm dh -> b nh s dh\")\n",
    "        K = einops.einsum(embed, self.W_K, \"b s dm, ng dm dh -> b ng s dh\")\n",
    "        V = einops.einsum(embed, self.W_V, \"b s dm, ng dm dh -> b ng s dh\")\n",
    "        # head_to_group = t.arange(0, self.num_head) % self.num_groups\n",
    "        K_t = einops.rearrange(K, \"b ng s dh -> b ng dh s\")\n",
    "        group_repeats = self.num_heads // self.num_groups\n",
    "        K_t_interleaved = K_t.repeat_interleave(repeats=group_repeats, dim=1)\n",
    "        # RuntimeError: einsum(): subscript b has size 8 for operand 1 which does not broadcast with previously seen size 4\n",
    "        Q_K = (\n",
    "            einops.einsum(Q, K_t_interleaved, \"b nh s_q dh, b nh dh s_k -> b nh s_q s_k\")\n",
    "            / t.sqrt(t.tensor(self.d_head))\n",
    "        )\n",
    "        softmaxed = t.softmax(Q_K, dim=-1)\n",
    "        V_interleaved = V.repeat_interleave(repeats = group_repeats, dim=1)\n",
    "        attn = einops.einsum(softmaxed, V_interleaved, \"b nh s_q s_k,  b nh s_k dh -> b nh s_q dh\")\n",
    "        attn_unraveled = einops.rearrange(attn, \"b nh s_q dh -> b s_q (nh dh)\") # TODO: is this order correct?\n",
    "        output = einops.einsum(attn_unraveled, self.W_O, \"b s_q nh_dh, nh_dh dm -> b s_q dm\")\n",
    "        \n",
    "        expected = F.scaled_dot_product_attention(\n",
    "            query=Q,\n",
    "            key=K,\n",
    "            value=V,\n",
    "            enable_gqa=True,\n",
    "        )\n",
    "        assert t.allclose(expected, attn)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "b97ca77b",
   "metadata": {},
   "outputs": [],
   "source": [
    "mha = GQA(\n",
    "    d_model=3, d_head=2, num_heads=4, num_groups=2,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "a67d4966",
   "metadata": {},
   "outputs": [],
   "source": [
    "resid = t.tensor(\n",
    "    [\n",
    "        [\n",
    "            [1.2, 2.3, 3.4],\n",
    "            [8.7, 7.6, 6.5],\n",
    "            [5.9, 9.2, 4.2],\n",
    "        ]\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "fa4cc0fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 4, 3, 2]) torch.Size([1, 4, 2, 3])\n",
      "torch.Size([1, 4, 3, 2]) torch.Size([1, 4, 3, 2])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.6028, 14.6338,  4.0317],\n",
       "         [-0.6007, 14.6337,  4.0316],\n",
       "         [-0.6007, 14.6337,  4.0316]]], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mha(resid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "963cf0a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_heads = 6\n",
    "num_groups = 2\n",
    "head_to_group = t.arange(0, num_heads) % num_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9ead454a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 0, 1, 0, 1])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "head_to_group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "d8a89138",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (nh, s, dh)  (ng, s, dh)\n",
    "# (6, 2, 1), (3, 1, 2)\n",
    "# (6, 2, 1), (6, 1, 2) -> (6, 2, 2)\n",
    "q = t.randn(6, 2, 1)\n",
    "k_t = t.randn(3, 1, 2)\n",
    "rep_kt = k_t.repeat_interleave(repeats=6 // 3, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "b6487936",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 2, 2])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(q @ rep_kt).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d03edc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 1, 2])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "7441ff7a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (2,) (4,) ",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[83]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m4\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m7\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mValueError\u001b[39m: operands could not be broadcast together with shapes (2,) (4,) "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.array([2, 4]) * np.array([7, 8, 2, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14526550",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
